{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel, BertConfig, BertForSequenceClassification, TextDataset, BertForNextSentencePrediction\n",
    "from transformers import AdamW, PreTrainedTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"conll_scenes_speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(x):\n",
    "    \n",
    "    if x == \"scene_start\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create training data\n",
    "X = []\n",
    "Y = []\n",
    "context = 200\n",
    "for fname in files[1:]:\n",
    "    \n",
    "    data = pd.read_csv(\"conll_scenes_speech/\"+fname, sep=\"\\t\", index_col=0)\n",
    "    sent_indexes = data[data.sentstart == \"yes\"].index\n",
    "    for index in sent_indexes[2:]:\n",
    "        try:\n",
    "            string = \" \".join(data.iloc[index-context:index-1,0])+\" [SEP] \"+\" \".join(data.iloc[index:index+context,0])\n",
    "            label = transform_label(data.iloc[index,3])\n",
    "            X.append(string)\n",
    "            Y.append(label)\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create eval data\n",
    "X_val = []\n",
    "Y_val = []\n",
    "for fname in files[:1]:\n",
    "    \n",
    "    data = pd.read_csv(\"conll_scenes_speech/\"+fname, sep=\"\\t\", index_col=0)\n",
    "    sent_indexes = data[data.sentstart == \"yes\"].index\n",
    "    for index in sent_indexes[2:]:\n",
    "        try:\n",
    "            string = \" \".join(data.iloc[index-context:index-1,0])+\" [SEP] \"+\" \".join(data.iloc[index:index+context,0])\n",
    "            label = transform_label(data.iloc[index,3])\n",
    "            X_val.append(string)\n",
    "            Y_val.append(label)\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# balance borders in training examples\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_sample(np.array(X).reshape(-1,1), np.array(Y).reshape(-1,1))\n",
    "X_rus, y_rus = shuffle(X_rus,y_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer + padding\n",
    "def create_example(x, maxlen):\n",
    "    input_ids = []\n",
    "    mask = []\n",
    "    for exp in x:\n",
    "        encoded = tokenizer.encode_plus(exp[0], padding=True)\n",
    "        padding_need = maxlen-len(encoded[\"input_ids\"])\n",
    "        if padding_need > 0:\n",
    "            input_ids.append((encoded[\"input_ids\"]+[0]*padding_need))\n",
    "            mask.append((encoded[\"attention_mask\"]+[0]*padding_need))\n",
    "        else:\n",
    "            input_ids.append((encoded[\"input_ids\"][:512]))\n",
    "            mask.append((encoded[\"attention_mask\"][:512]))\n",
    "    mask = torch.tensor(mask)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    return input_ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute tokenizer + padding\n",
    "input_ids, masks = create_example(X_rus, 512)\n",
    "labels = torch.tensor(y_rus)\n",
    "input_ids_val, masks_val = create_example(X_val, 512)\n",
    "labels_val = torch.tensor(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training batches\n",
    "batches = []\n",
    "bsize = 8\n",
    "for b in list(range(0,len(labels)-bsize,bsize)):\n",
    "    \n",
    "    if len(input_ids[b:b+bsize]) != bsize:\n",
    "        break\n",
    "    batches.append([input_ids[b:b+bsize],masks[b:b+bsize],labels[b:b+bsize]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval batches\n",
    "val_batches = []\n",
    "for b in list(range(0,len(labels_val)-bsize,bsize)):\n",
    "    \n",
    "    if len(input_ids_val[b:b+bsize]) != bsize:\n",
    "        break\n",
    "    val_batches.append([input_ids_val[b:b+bsize],masks_val[b:b+bsize],labels_val[b:b+bsize]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(pred_flat,labels_flat, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "total_steps = len(batches) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 10 1.2292439699172975\n",
      "Batch Loss: 20 0.9781380504369735\n",
      "Batch Loss: 30 0.8999764561653137\n",
      "Batch Loss: 40 0.8780328065156937\n",
      "Batch Loss: 50 0.8661887979507447\n",
      "Batch Loss: 60 0.8665582756201426\n",
      "Batch Loss: 70 0.8448717389787946\n",
      "Batch Loss: 80 0.8263776428997517\n",
      "Batch Loss: 90 0.8204327344894409\n",
      "Batch Loss: 100 0.8215398174524308\n",
      "Batch Loss: 110 0.8220973898064007\n",
      "Batch Loss: 120 0.8152921517690023\n",
      "Batch Loss: 130 0.8101320550991938\n",
      "Batch Loss: 140 0.8055408477783204\n",
      "Batch Loss: 150 0.8098158299922943\n",
      "Batch Loss: 160 0.8037900768220425\n",
      "Batch Loss: 170 0.7971783921999089\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7249630749225616\n",
      "Batch Loss: 20 0.7136924237012863\n",
      "Batch Loss: 30 0.7310001869996389\n",
      "Batch Loss: 40 0.7617949590086937\n",
      "Batch Loss: 50 0.7546090054512024\n",
      "Batch Loss: 60 0.7574657211701076\n",
      "Batch Loss: 70 0.7552310909543719\n",
      "Batch Loss: 80 0.7518838889896869\n",
      "Batch Loss: 90 0.7567178686459859\n",
      "Batch Loss: 100 0.7567897325754166\n",
      "Batch Loss: 110 0.7562584925781597\n",
      "Batch Loss: 120 0.7654891714453698\n",
      "Batch Loss: 130 0.7751582860946655\n",
      "Batch Loss: 140 0.7732923286301749\n",
      "Batch Loss: 150 0.7696823259194692\n",
      "Batch Loss: 160 0.7661866251379251\n",
      "Batch Loss: 170 0.762041651501375\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7360370635986329\n",
      "Batch Loss: 20 0.7182235449552536\n",
      "Batch Loss: 30 0.7405425846576691\n",
      "Batch Loss: 40 0.7495069652795792\n",
      "Batch Loss: 50 0.7395683169364929\n",
      "Batch Loss: 60 0.7416724900404612\n",
      "Batch Loss: 70 0.7438660374709538\n",
      "Batch Loss: 80 0.7457427501678466\n",
      "Batch Loss: 90 0.7539438929822709\n",
      "Batch Loss: 100 0.753884152173996\n",
      "Batch Loss: 110 0.7550659699873491\n",
      "Batch Loss: 120 0.7503285343448322\n",
      "Batch Loss: 130 0.7463895563895886\n",
      "Batch Loss: 140 0.7416453216757093\n",
      "Batch Loss: 150 0.7402036245663961\n",
      "Batch Loss: 160 0.7387349169701338\n",
      "Batch Loss: 170 0.7361687884611242\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7170868217945099\n",
      "Batch Loss: 20 0.7081587523221969\n",
      "Batch Loss: 30 0.7074432810147603\n",
      "Batch Loss: 40 0.7192939907312393\n",
      "Batch Loss: 50 0.7174487006664276\n",
      "Batch Loss: 60 0.7274216681718826\n",
      "Batch Loss: 70 0.7243335672787258\n",
      "Batch Loss: 80 0.7210701525211334\n",
      "Batch Loss: 90 0.7247472710079617\n",
      "Batch Loss: 100 0.723943492770195\n",
      "Batch Loss: 110 0.7258851159702647\n",
      "Batch Loss: 120 0.7254885743061702\n",
      "Batch Loss: 130 0.7237904658684364\n",
      "Batch Loss: 140 0.720730300460543\n",
      "Batch Loss: 150 0.7200463930765788\n",
      "Batch Loss: 160 0.7193177405744791\n",
      "Batch Loss: 170 0.7177878453451044\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7169537484645844\n",
      "Batch Loss: 20 0.7083571761846542\n",
      "Batch Loss: 30 0.7124372939268748\n",
      "Batch Loss: 40 0.710690227150917\n",
      "Batch Loss: 50 0.7154987156391144\n",
      "Batch Loss: 60 0.7176799754301707\n",
      "Batch Loss: 70 0.7155223906040191\n",
      "Batch Loss: 80 0.7137652181088925\n",
      "Batch Loss: 90 0.717611167828242\n",
      "Batch Loss: 100 0.7162739020586014\n",
      "Batch Loss: 110 0.7158335945822976\n",
      "Batch Loss: 120 0.7200835073987643\n",
      "Batch Loss: 130 0.7219307358448322\n",
      "Batch Loss: 140 0.7199463235480444\n",
      "Batch Loss: 150 0.7195454176266988\n",
      "Batch Loss: 160 0.7196064330637455\n",
      "Batch Loss: 170 0.7211858409292558\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.723381119966507\n",
      "Batch Loss: 20 0.7115377873182297\n",
      "Batch Loss: 30 0.7107420921325683\n",
      "Batch Loss: 40 0.7093289405107498\n",
      "Batch Loss: 50 0.7075157308578491\n",
      "Batch Loss: 60 0.7144620696703593\n",
      "Batch Loss: 70 0.7124353221484593\n",
      "Batch Loss: 80 0.7158846966922283\n",
      "Batch Loss: 90 0.7138712061776056\n",
      "Batch Loss: 100 0.7128507602214813\n",
      "Batch Loss: 110 0.7134940927678889\n",
      "Batch Loss: 120 0.7141420086224873\n",
      "Batch Loss: 130 0.7127094493462489\n",
      "Batch Loss: 140 0.7101753967148917\n",
      "Batch Loss: 150 0.7127656129995982\n",
      "Batch Loss: 160 0.7136228196322918\n",
      "Batch Loss: 170 0.7144122828455532\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7532281279563904\n",
      "Batch Loss: 20 0.7353643536567688\n",
      "Batch Loss: 30 0.736393125851949\n",
      "Batch Loss: 40 0.7265800982713699\n",
      "Batch Loss: 50 0.7195538425445557\n",
      "Batch Loss: 60 0.7191148102283478\n",
      "Batch Loss: 70 0.7173551278454917\n",
      "Batch Loss: 80 0.7233559288084507\n",
      "Batch Loss: 90 0.7196987291177114\n",
      "Batch Loss: 100 0.7189175873994827\n",
      "Batch Loss: 110 0.7215152539990165\n",
      "Batch Loss: 120 0.7228887553016344\n",
      "Batch Loss: 130 0.7223465396807744\n",
      "Batch Loss: 140 0.7188898657049452\n",
      "Batch Loss: 150 0.7196332172552744\n",
      "Batch Loss: 160 0.7193811114877462\n",
      "Batch Loss: 170 0.7169772432130925\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7223672449588776\n",
      "Batch Loss: 20 0.7094672530889511\n",
      "Batch Loss: 30 0.708347866932551\n",
      "Batch Loss: 40 0.7060862019658088\n",
      "Batch Loss: 50 0.7041521513462067\n",
      "Batch Loss: 60 0.7046868999799093\n",
      "Batch Loss: 70 0.7046590166432517\n",
      "Batch Loss: 80 0.7116439275443553\n",
      "Batch Loss: 90 0.709893403450648\n",
      "Batch Loss: 100 0.7102056628465653\n",
      "Batch Loss: 110 0.7109446915713223\n",
      "Batch Loss: 120 0.7115976919730505\n",
      "Batch Loss: 130 0.7105518428178934\n",
      "Batch Loss: 140 0.7080386830227715\n",
      "Batch Loss: 150 0.7137404676278433\n",
      "Batch Loss: 160 0.7140895958989859\n",
      "Batch Loss: 170 0.7121334580814137\n",
      "validation acc: 0.8875127733496835\n",
      "Batch Loss: 10 0.7025331020355224\n",
      "Batch Loss: 20 0.6986782103776932\n",
      "Batch Loss: 30 0.699175359805425\n",
      "Batch Loss: 40 0.6977165073156357\n",
      "Batch Loss: 50 0.6966941452026367\n",
      "Batch Loss: 60 0.6966519425312678\n",
      "Batch Loss: 70 0.6978110449654715\n",
      "Batch Loss: 80 0.7011971913278103\n",
      "Batch Loss: 90 0.7005325257778168\n",
      "Batch Loss: 100 0.7003835785388947\n",
      "Batch Loss: 110 0.7012453897432848\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    total_loss = 0 \n",
    "    model.train() \n",
    "    i = 0\n",
    "    for step, batch in enumerate(batches):\n",
    "        i+=1\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "   \n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, next_sentence_label=b_labels)\n",
    "\n",
    "\n",
    "        loss = outputs[0] \n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step() \n",
    "        scheduler.step()\n",
    "        avg_train_loss = total_loss / i \n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"Batch Loss: \"+str(i)+\" \"+str(avg_train_loss))\n",
    "        \n",
    "        \n",
    "        \n",
    "    nb_eval_steps = 0\n",
    "    eval_accuracy = 0\n",
    "    for batch in val_batches:\n",
    "\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0] \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy \n",
    "            nb_eval_steps += 1 \n",
    "    print(\"validation acc: \"+str(eval_accuracy/nb_eval_steps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
